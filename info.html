<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<link rel="icon" href="./favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		
		<link href="./_app/immutable/assets/app.27bee442.css" rel="stylesheet">
		<link rel="modulepreload" href="./_app/immutable/entry/start.4d3ed595.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/scheduler.9eabb343.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/singletons.a0b85afd.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/index.8d5d2cfe.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/paths.d4e5d7fc.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/control.f5b05b5f.js">
		<link rel="modulepreload" href="./_app/immutable/entry/app.9aa5569f.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/index.3fd3c9c8.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/0.70f78e83.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/4.00675a87.js">
	</head>
	<body data-sveltekit-preload-data="hover">
		<div style="display: contents">  <div class="help-button" data-svelte-h="svelte-1uz5799"><a href="/info"><img src="/help.png"></a></div> <div class="home-button" data-svelte-h="svelte-62ahtl"><a href="/"><img src="/home.png"></a></div> <div class="pt-5 pl-20 pb-10 pr-20" data-svelte-h="svelte-1sl9ank"><h1>Where did this data come from?</h1> <p class="home-text">In order to demonstrate a prototype of this visualization tool, we used the CounterFact dataset 
        <a href="https://rome.baulab.info/" class="highlighted-link">(Meng et al., 2022)</a> as a starting point to get a list of entities.
        This dataset contains about 20 thousand prompts with known entities and facts about them. 
        <br><br>
        Previous work from <a href="https://rome.baulab.info/">Meng et al. (2022)</a> and <a href="https://arxiv.org/abs/2304.14767" class="highlighted-link">Geva et al. (2023)</a> suggests 
        that for multi-token entities, Transformer models collect factual information about entities in early layers at the last token position of that entity (e.g. early representations of 
        &quot;Wars&quot; in &quot;Star Wars&quot;). Our results show that simple linear probes trained to predict &quot;Wars&quot; from &quot;Star&quot; actually systematically fail for CounterFact 
        entities (with an accuracy of 0.4%), even though the same probes can predict regular bigrams quite reliably (&quot;is&quot; from &quot;He&quot;).
    
    <br><br><br> </p><h2>Where did this plot come from?</h2> <p class="home-text">For CounterFact prompts that Llama-2 was able to answer correctly, we take the hidden representations of the <i>last</i> token for each
        entity and perform dimensionality reduction to get a 2D vector describing that hidden state&#39;s similarity to other concept hidden states, 
        resulting in the scatterplot shown on this website. Currently, the data being displayed is a tSNE plot of representations at the <i>fifth</i> layer of Llama-2. 
        Confidence scores are currently placeholders. Euclidean distance measures how far any given point is to another.</p></div> 
			
			<script>
				{
					__sveltekit_1nocd3p = {
						base: new URL(".", location).pathname.slice(0, -1),
						env: {}
					};

					const element = document.currentScript.parentElement;

					const data = [null,null];

					Promise.all([
						import("./_app/immutable/entry/start.4d3ed595.js"),
						import("./_app/immutable/entry/app.9aa5569f.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 4],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
