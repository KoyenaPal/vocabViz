{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11bdf68b-f721-473e-95c7-5ccf5077e4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /disk/u/koyena/miniconda3/envs/engine/lib/python3.10/site-packages (5.18.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /disk/u/koyena/miniconda3/envs/engine/lib/python3.10/site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: packaging in /disk/u/koyena/miniconda3/envs/engine/lib/python3.10/site-packages (from plotly) (23.1)\n",
      "Requirement already satisfied: scikit-learn in /disk/u/koyena/miniconda3/envs/engine/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /disk/u/koyena/miniconda3/envs/engine/lib/python3.10/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /disk/u/koyena/miniconda3/envs/engine/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /disk/u/koyena/miniconda3/envs/engine/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /disk/u/koyena/miniconda3/envs/engine/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "261f83a8-4fd4-4106-a453-995a9259a8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 14:36:11.633784: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-05 14:36:11.676418: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-05 14:36:12.379876: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff518eb26f4474a84e20c4fb7f6d586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/u/koyena/miniconda3/envs/engine/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import torch, baukit\n",
    "from baukit import TraceDict\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from nnsight import LanguageModel\n",
    "#model = LanguageModel(\"meta-llama/Llama-2-7b-hf\", token=\"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\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", token=\"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\")\n",
    "model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", token=\"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\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12b0e0ad-095b-4966-a703-c250b7f4786e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e5e3f8-0fdb-4e3b-b97f-b88bebd8184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_hidden_states(model, tok, prefix, layer=31, device=\"cuda\"):\n",
    "    inp = {k: torch.tensor(v)[None].to(device) for k, v in tok(prefix).items()}\n",
    "    layer_names = [n for n, _ in model.named_modules()\n",
    "                   if re.match(f'^model.layers.{layer}$', n)]\n",
    "    with TraceDict(model, layer_names) as tr:\n",
    "        logits = model(**inp)['logits']\n",
    "    return tr[layer_names[0]].output[0][:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d3b3f69-0ebd-4011-a537-d7f11dc12780",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = pd.read_csv(\"../data/counterfact.csv\").subject.values\n",
    "prompt_list = pd.read_csv(\"../data/counterfact.csv\").prompt.values\n",
    "target_true_list = pd.read_csv(\"../data/counterfact.csv\").target_true.values\n",
    "\n",
    "layer = 0\n",
    "hidden_states = []\n",
    "for data, prompt, target in zip(data_list, prompt_list, target_true_list):\n",
    "    input_prompt = str(prompt).replace(\"{}\", data)\n",
    "    inp = {k: torch.tensor(v)[None].to('cuda') for k, v in tokenizer(input_prompt).items()}\n",
    "    outputs = model.generate(**inp,max_new_tokens=3)\n",
    "    output_text = tokenizer.batch_decode(outputs)[0]\n",
    "    if target in output_text:\n",
    "        curr_hidden_state = get_layer_hidden_states(model, tokenizer, data, layer=layer, device=\"cuda\")\n",
    "        hidden_states.append(curr_hidden_state.cpu().detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e44834b1-14db-4f52-b789-9c6cdd7adb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6615"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "210d60b6-e347-4a63-977e-c9cbf29ef9ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m tsne \u001b[38;5;241m=\u001b[39m PCA(n_components, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m data_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([curr_hidden_states \u001b[38;5;28;01mfor\u001b[39;00m curr_hidden_states \u001b[38;5;129;01min\u001b[39;00m hidden_states])\n\u001b[0;32m----> 6\u001b[0m principal_components \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241m.\u001b[39mfit_transform(data_array)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#principal_components = tsne.fit_transform(data_array)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create a DataFrame with the principal components\u001b[39;00m\n\u001b[1;32m      9\u001b[0m PC1 \u001b[38;5;241m=\u001b[39m principal_components[:,\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pca' is not defined"
     ]
    }
   ],
   "source": [
    "#from sklearn.manifold import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "n_components = 2\n",
    "pca = PCA(n_components, random_state=0)\n",
    "data_array = np.stack([curr_hidden_states for curr_hidden_states in hidden_states])\n",
    "principal_components = pca.fit_transform(data_array)\n",
    "#principal_components = tsne.fit_transform(data_array)\n",
    "# Create a DataFrame with the principal components\n",
    "PC1 = principal_components[:,0]\n",
    "PC2 = principal_components[:,1]\n",
    "labels = data_list.astype(str)\n",
    "zipped = list(zip(PC1, \n",
    "                  PC2,\n",
    "                  labels))\n",
    "\n",
    "pc_df = pd.DataFrame(zipped, \n",
    "                     columns=['PC1', \n",
    "                              'PC2', \n",
    "                              'Labels'])\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    " \n",
    "sns.scatterplot(data=pc_df, \n",
    "                x=\"PC1\", \n",
    "                y=\"PC2\")\n",
    " \n",
    "plt.title(f\"PCA at Layer {layer}\",\n",
    "          fontsize=16)\n",
    "plt.xlabel('First Principal Component',\n",
    "           fontsize=16)\n",
    "plt.ylabel('Second Principal Component',\n",
    "           fontsize=16)\n",
    "\n",
    "coordinates = []\n",
    "def label_point(x, y, val, ax):\n",
    "    a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)\n",
    "    for i, point in a.iterrows():\n",
    "        coordinates.append((point['x'], point['y'], str(point['val'])))\n",
    "        # ax.text(point['x']+.02, point['y'], str(point['val']))\n",
    "\n",
    "label_point(pc_df.PC1, pc_df.PC2, pc_df.Labels, plt.gca()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6757b2cf-fbde-48c6-9f23-88034a552a57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03f37a-a47e-4c72-aee4-4352c76ba410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(f'pca_coordinates_layer_{layer}.csv', 'w') as f:\n",
    "     \n",
    "    # using csv.writer method from CSV package\n",
    "    write = csv.writer(f)\n",
    "     \n",
    "    write.writerow([\"x\",\"y\",\"value\"])\n",
    "    write.writerows(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ae12f7-a7f6-47d6-bcc2-b0f789036581",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"../data/counterfact.csv\")\n",
    "print(data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9603abbd-a0cb-4c36-b25c-48483ac81497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "def find_closest_points(coordinates, target_x, target_y):\n",
    "    closest_points = []\n",
    "    min_distance = [float('inf')]\n",
    "    for current_x, current_y, value in coordinates:\n",
    "        distance = math.sqrt((current_x - target_x)**2 + (current_y - target_y)**2)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_points = [(x, y, value)]\n",
    "        elif distance == min_distance:\n",
    "            closest_points.append((x, y, value))\n",
    "    return closest_points,\n",
    "\n",
    "point_coordinates = np.array([[point[0],point[1]] for point in coordinates])\n",
    "kdtree = KDTree(point_coordinates)\n",
    "json_dict_list = []\n",
    "\n",
    "for index, row in data_df.iterrows():\n",
    "    print(index)\n",
    "    curr_dict = {}\n",
    "    curr_dict[\"id\"] = row[\"case_id\"]\n",
    "    curr_dict[\"entry\"] = row[\"subject\"]\n",
    "    curr_dict[\"confidence\"] = random.randint(50, 100)/100\n",
    "    curr_dict[\"vector\"] = [coordinates[index][0], coordinates[index][1]]\n",
    "    example_sent = str(row[\"prompt\"]).replace(\"{}\", row[\"subject\"]) + \" \" + str(row[\"target_true\"])\n",
    "    curr_dict[\"examples\"] = [{\"doc\":row[\"case_id\"], \"sentence\":example_sent}]\n",
    "    query_point = curr_dict[\"vector\"]\n",
    "    distances, indexes = kdtree.query(query_point, k=len(coordinates))\n",
    "    three_closest_points = indexes[1:4]\n",
    "    three_closest_distances = distances[1:4]\n",
    "    \n",
    "    three_furthest_points = indexes[-3:]\n",
    "    three_furthest_distances = distances[-3:]\n",
    "    \n",
    "    closest_list = []\n",
    "    for curr_point, curr_dist in zip(three_closest_points,three_closest_distances):\n",
    "        curr_closest_dict = {}\n",
    "        curr_closest_dict[\"id\"] = curr_point\n",
    "        curr_closest_dict[\"distance\"] = curr_dist\n",
    "        curr_closest_dict[\"entry\"] = coordinates[curr_point][2]\n",
    "        closest_list.append(curr_closest_dict)\n",
    "    \n",
    "    curr_dict[\"close\"] = closest_list\n",
    "    \n",
    "    furthest_list = []\n",
    "    for curr_point, curr_dist in zip(three_furthest_points,three_furthest_distances):\n",
    "        curr_furthest_dict = {}\n",
    "        curr_furthest_dict[\"id\"] = curr_point\n",
    "        curr_furthest_dict[\"distance\"] = curr_dist\n",
    "        curr_furthest_dict[\"entry\"] = coordinates[curr_point][2]\n",
    "        furthest_list.append(curr_furthest_dict)\n",
    "        \n",
    "    curr_dict[\"distant\"] = furthest_list\n",
    "    json_dict_list.append(curr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed20e36-8d5d-4b54-bd44-b62a3761636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    " \n",
    "with open(f'../data/pca_correct_only_layer_{layer}.json', 'w') as f:\n",
    "    json.dump(json_dict_list , f, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456e5f3e-92ab-4fb3-b5fe-225070dfcf65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
